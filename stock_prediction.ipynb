{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from yahoo_fin import stock_info as si\n",
    "from collections import deque\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed, so we can get the same results after rerunning several times\n",
    "np.random.seed(314)\n",
    "tf.random.set_seed(314)\n",
    "random.seed(314)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "# Window size or the sequence length\n",
    "N_STEPS = 50\n",
    "# Lookup step, 1 is the next day\n",
    "LOOKUP_STEP = 15\n",
    "\n",
    "# whether to scale feature columns & output price as well\n",
    "SCALE = True\n",
    "scale_str = f\"sc-{int(SCALE)}\"\n",
    "# whether to shuffle the dataset\n",
    "SHUFFLE = True\n",
    "shuffle_str = f\"sh-{int(SHUFFLE)}\"\n",
    "# whether to split the training/testing set by date\n",
    "SPLIT_BY_DATE = False\n",
    "split_by_date_str = f\"sbd-{int(SPLIT_BY_DATE)}\"\n",
    "# test ratio size, 0.2 is 20%\n",
    "TEST_SIZE = 0.2\n",
    "# features to use\n",
    "FEATURE_COLUMNS = [\"adjclose\", \"volume\", \"open\", \"high\", \"low\"]\n",
    "# date now\n",
    "date_now = time.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "### model parameters\n",
    "\n",
    "N_LAYERS = 2\n",
    "# LSTM cell\n",
    "CELL = LSTM\n",
    "# 256 LSTM neurons\n",
    "UNITS = 256\n",
    "# 40% dropout\n",
    "DROPOUT = 0.4\n",
    "# whether to use bidirectional RNNs\n",
    "BIDIRECTIONAL = False\n",
    "\n",
    "### training parameters\n",
    "\n",
    "# mean absolute error loss\n",
    "# LOSS = \"mae\"\n",
    "# huber loss\n",
    "LOSS = \"huber_loss\"\n",
    "OPTIMIZER = \"adam\"\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 5\n",
    "\n",
    "# Amazon stock market\n",
    "ticker = \"AMZN\"\n",
    "ticker_data_filename = os.path.join(\"data\", f\"{ticker}_{date_now}.csv\")\n",
    "# model name to save, making it as unique as possible based on parameters\n",
    "model_name = f\"{date_now}_{ticker}-{shuffle_str}-{scale_str}-{split_by_date_str}-\\\n",
    "{LOSS}-{OPTIMIZER}-{CELL.__name__}-seq-{N_STEPS}-step-{LOOKUP_STEP}-layers-{N_LAYERS}-units-{UNITS}\"\n",
    "if BIDIRECTIONAL:\n",
    "    model_name += \"-b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_in_unison(a, b):\n",
    "    # shuffle two arrays in the same way\n",
    "    state = np.random.get_state()\n",
    "    np.random.shuffle(a)\n",
    "    np.random.set_state(state)\n",
    "    np.random.shuffle(b)\n",
    "\n",
    "\n",
    "def load_data(ticker, n_steps=50, scale=True, shuffle=True, lookup_step=1, split_by_date=True,\n",
    "                test_size=0.2, feature_columns=['adjclose', 'volume', 'open', 'high', 'low']):\n",
    "    \"\"\"\n",
    "    Loads data from Yahoo Finance source, as well as scaling, shuffling, normalizing and splitting.\n",
    "    Params:\n",
    "        ticker (str/pd.DataFrame): the ticker you want to load, examples include AAPL, TESL, etc.\n",
    "        n_steps (int): the historical sequence length (i.e window size) used to predict, default is 50\n",
    "        scale (bool): whether to scale prices from 0 to 1, default is True\n",
    "        shuffle (bool): whether to shuffle the dataset (both training & testing), default is True\n",
    "        lookup_step (int): the future lookup step to predict, default is 1 (e.g next day)\n",
    "        split_by_date (bool): whether we split the dataset into training/testing by date, setting it \n",
    "            to False will split datasets in a random way\n",
    "        test_size (float): ratio for test data, default is 0.2 (20% testing data)\n",
    "        feature_columns (list): the list of features to use to feed into the model, default is everything grabbed from yahoo_fin\n",
    "    \"\"\"\n",
    "    # see if ticker is already a loaded stock from yahoo finance\n",
    "    if isinstance(ticker, str):\n",
    "        # load it from yahoo_fin library\n",
    "        df = si.get_data(ticker)\n",
    "    elif isinstance(ticker, pd.DataFrame):\n",
    "        # already loaded, use it directly\n",
    "        df = ticker\n",
    "    else:\n",
    "        raise TypeError(\"ticker can be either a str or a `pd.DataFrame` instances\")\n",
    "\n",
    "    # this will contain all the elements we want to return from this function\n",
    "    result = {}\n",
    "    # we will also return the original dataframe itself\n",
    "    result['df'] = df.copy()\n",
    "\n",
    "    # make sure that the passed feature_columns exist in the dataframe\n",
    "    for col in feature_columns:\n",
    "        assert col in df.columns, f\"'{col}' does not exist in the dataframe.\"\n",
    "\n",
    "    # add date as a column\n",
    "    if \"date\" not in df.columns:\n",
    "        df[\"date\"] = df.index\n",
    "\n",
    "    if scale:\n",
    "        column_scaler = {}\n",
    "        # scale the data (prices) from 0 to 1\n",
    "        for column in feature_columns:\n",
    "            scaler = preprocessing.MinMaxScaler()\n",
    "            df[column] = scaler.fit_transform(np.expand_dims(df[column].values, axis=1))\n",
    "            column_scaler[column] = scaler\n",
    "\n",
    "        # add the MinMaxScaler instances to the result returned\n",
    "        result[\"column_scaler\"] = column_scaler\n",
    "\n",
    "    # add the target column (label) by shifting by `lookup_step`\n",
    "    df['future'] = df['adjclose'].shift(-lookup_step)\n",
    "\n",
    "    # last `lookup_step` columns contains NaN in future column\n",
    "    # get them before droping NaNs\n",
    "    last_sequence = np.array(df[feature_columns].tail(lookup_step))\n",
    "    \n",
    "    # drop NaNs\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    sequence_data = []\n",
    "    sequences = deque(maxlen=n_steps)\n",
    "\n",
    "    for entry, target in zip(df[feature_columns + [\"date\"]].values, df['future'].values):\n",
    "        sequences.append(entry)\n",
    "        if len(sequences) == n_steps:\n",
    "            sequence_data.append([np.array(sequences), target])\n",
    "\n",
    "    # get the last sequence by appending the last `n_step` sequence with `lookup_step` sequence\n",
    "    # for instance, if n_steps=50 and lookup_step=10, last_sequence should be of 60 (that is 50+10) length\n",
    "    # this last_sequence will be used to predict future stock prices that are not available in the dataset\n",
    "    last_sequence = list([s[:len(feature_columns)] for s in sequences]) + list(last_sequence)\n",
    "    last_sequence = np.array(last_sequence).astype(np.float32)\n",
    "    # add to result\n",
    "    result['last_sequence'] = last_sequence\n",
    "    \n",
    "    # construct the X's and y's\n",
    "    X, y = [], []\n",
    "    for seq, target in sequence_data:\n",
    "        X.append(seq)\n",
    "        y.append(target)\n",
    "\n",
    "    # convert to numpy arrays\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "\n",
    "    if split_by_date:\n",
    "        # split the dataset into training & testing sets by date (not randomly splitting)\n",
    "        train_samples = int((1 - test_size) * len(X))\n",
    "        result[\"X_train\"] = X[:train_samples]\n",
    "        result[\"y_train\"] = y[:train_samples]\n",
    "        result[\"X_test\"]  = X[train_samples:]\n",
    "        result[\"y_test\"]  = y[train_samples:]\n",
    "        if shuffle:\n",
    "            # shuffle the datasets for training (if shuffle parameter is set)\n",
    "            shuffle_in_unison(result[\"X_train\"], result[\"y_train\"])\n",
    "            shuffle_in_unison(result[\"X_test\"], result[\"y_test\"])\n",
    "    else:    \n",
    "        # split the dataset randomly\n",
    "        result[\"X_train\"], result[\"X_test\"], result[\"y_train\"], result[\"y_test\"] = train_test_split(X, y, \n",
    "                                                                                test_size=test_size, shuffle=shuffle)\n",
    "\n",
    "    # get the list of test set dates\n",
    "    dates = result[\"X_test\"][:, -1, -1]\n",
    "    # retrieve test features from the original dataframe\n",
    "    result[\"test_df\"] = result[\"df\"].loc[dates]\n",
    "    # remove duplicated dates in the testing dataframe\n",
    "    result[\"test_df\"] = result[\"test_df\"][~result[\"test_df\"].index.duplicated(keep='first')]\n",
    "    # remove dates from the training/testing sets & convert to float32\n",
    "    result[\"X_train\"] = result[\"X_train\"][:, :, :len(feature_columns)].astype(np.float32)\n",
    "    result[\"X_test\"] = result[\"X_test\"][:, :, :len(feature_columns)].astype(np.float32)\n",
    "\n",
    "    return result,df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(sequence_length, n_features, units=256, cell=LSTM, n_layers=2, dropout=0.3,\n",
    "                loss=\"mean_absolute_error\", optimizer=\"rmsprop\", bidirectional=False):\n",
    "    model = Sequential()\n",
    "    for i in range(n_layers):\n",
    "        if i == 0:\n",
    "            # first layer\n",
    "            if bidirectional:\n",
    "                model.add(Bidirectional(cell(units, return_sequences=True), batch_input_shape=(None, sequence_length, n_features)))\n",
    "            else:\n",
    "                model.add(cell(units, return_sequences=True, batch_input_shape=(None, sequence_length, n_features)))\n",
    "        elif i == n_layers - 1:\n",
    "            # last layer\n",
    "            if bidirectional:\n",
    "                model.add(Bidirectional(cell(units, return_sequences=False)))\n",
    "            else:\n",
    "                model.add(cell(units, return_sequences=False))\n",
    "        else:\n",
    "            # hidden layers\n",
    "            if bidirectional:\n",
    "                model.add(Bidirectional(cell(units, return_sequences=True)))\n",
    "            else:\n",
    "                model.add(cell(units, return_sequences=True))\n",
    "        # add dropout after each layer\n",
    "        model.add(Dropout(dropout))\n",
    "    model.add(Dense(1, activation=\"linear\"))\n",
    "    model.compile(loss=loss, metrics=[\"mean_absolute_error\"], optimizer=optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# create these folders if they does not exist\n",
    "if not os.path.isdir(\"results\"):\n",
    "    os.mkdir(\"results\")\n",
    "\n",
    "if not os.path.isdir(\"logs\"):\n",
    "    os.mkdir(\"logs\")\n",
    "\n",
    "if not os.path.isdir(\"data\"):\n",
    "    os.mkdir(\"data\")\n",
    "\n",
    "# load the data\n",
    "data,df_stock = load_data(ticker, N_STEPS, scale=SCALE, split_by_date=SPLIT_BY_DATE, \n",
    "                shuffle=SHUFFLE, lookup_step=LOOKUP_STEP, test_size=TEST_SIZE, \n",
    "                feature_columns=FEATURE_COLUMNS)\n",
    "\n",
    "# save the dataframe\n",
    "data[\"df\"].to_csv(ticker_data_filename)\n",
    "\n",
    "# construct the model\n",
    "model = create_model(N_STEPS, len(FEATURE_COLUMNS), loss=LOSS, units=UNITS, cell=CELL, n_layers=N_LAYERS,\n",
    "                    dropout=DROPOUT, optimizer=OPTIMIZER, bidirectional=BIDIRECTIONAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>adjclose</th>\n",
       "      <th>volume</th>\n",
       "      <th>ticker</th>\n",
       "      <th>date</th>\n",
       "      <th>future</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1997-05-15</th>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.000279</td>\n",
       "      <td>0.000166</td>\n",
       "      <td>0.097917</td>\n",
       "      <td>0.000151</td>\n",
       "      <td>0.690172</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>1997-05-15</td>\n",
       "      <td>0.000070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-05-16</th>\n",
       "      <td>0.000150</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>0.086458</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.136869</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>1997-05-16</td>\n",
       "      <td>0.000078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-05-19</th>\n",
       "      <td>0.000095</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>0.085417</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.054117</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>1997-05-19</td>\n",
       "      <td>0.000050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-05-20</th>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>0.081771</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.047957</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>1997-05-20</td>\n",
       "      <td>0.000039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-05-21</th>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.071354</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.176865</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>1997-05-21</td>\n",
       "      <td>0.000056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-15</th>\n",
       "      <td>0.479986</td>\n",
       "      <td>0.476704</td>\n",
       "      <td>0.473034</td>\n",
       "      <td>88.449997</td>\n",
       "      <td>0.473887</td>\n",
       "      <td>0.036141</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>2022-12-15</td>\n",
       "      <td>0.468042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-16</th>\n",
       "      <td>0.471329</td>\n",
       "      <td>0.473416</td>\n",
       "      <td>0.469029</td>\n",
       "      <td>87.860001</td>\n",
       "      <td>0.470723</td>\n",
       "      <td>0.065677</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>2022-12-16</td>\n",
       "      <td>0.481501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-19</th>\n",
       "      <td>0.467268</td>\n",
       "      <td>0.464296</td>\n",
       "      <td>0.457015</td>\n",
       "      <td>84.919998</td>\n",
       "      <td>0.454959</td>\n",
       "      <td>0.035529</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>2022-12-19</td>\n",
       "      <td>0.509490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-20</th>\n",
       "      <td>0.455618</td>\n",
       "      <td>0.458887</td>\n",
       "      <td>0.456041</td>\n",
       "      <td>85.190002</td>\n",
       "      <td>0.456407</td>\n",
       "      <td>0.031107</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>2022-12-20</td>\n",
       "      <td>0.510455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-21</th>\n",
       "      <td>0.460160</td>\n",
       "      <td>0.462174</td>\n",
       "      <td>0.460803</td>\n",
       "      <td>86.769997</td>\n",
       "      <td>0.464879</td>\n",
       "      <td>0.023845</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>2022-12-21</td>\n",
       "      <td>0.525736</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6445 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                open      high       low      close  adjclose    volume  \\\n",
       "1997-05-15  0.000276  0.000279  0.000166   0.097917  0.000151  0.690172   \n",
       "1997-05-16  0.000150  0.000141  0.000107   0.086458  0.000089  0.136869   \n",
       "1997-05-19  0.000095  0.000086  0.000085   0.085417  0.000084  0.054117   \n",
       "1997-05-20  0.000086  0.000080  0.000087   0.081771  0.000064  0.047957   \n",
       "1997-05-21  0.000061  0.000052  0.000017   0.071354  0.000008  0.176865   \n",
       "...              ...       ...       ...        ...       ...       ...   \n",
       "2022-12-15  0.479986  0.476704  0.473034  88.449997  0.473887  0.036141   \n",
       "2022-12-16  0.471329  0.473416  0.469029  87.860001  0.470723  0.065677   \n",
       "2022-12-19  0.467268  0.464296  0.457015  84.919998  0.454959  0.035529   \n",
       "2022-12-20  0.455618  0.458887  0.456041  85.190002  0.456407  0.031107   \n",
       "2022-12-21  0.460160  0.462174  0.460803  86.769997  0.464879  0.023845   \n",
       "\n",
       "           ticker       date    future  \n",
       "1997-05-15   AMZN 1997-05-15  0.000070  \n",
       "1997-05-16   AMZN 1997-05-16  0.000078  \n",
       "1997-05-19   AMZN 1997-05-19  0.000050  \n",
       "1997-05-20   AMZN 1997-05-20  0.000039  \n",
       "1997-05-21   AMZN 1997-05-21  0.000056  \n",
       "...           ...        ...       ...  \n",
       "2022-12-15   AMZN 2022-12-15  0.468042  \n",
       "2022-12-16   AMZN 2022-12-16  0.481501  \n",
       "2022-12-19   AMZN 2022-12-19  0.509490  \n",
       "2022-12-20   AMZN 2022-12-20  0.510455  \n",
       "2022-12-21   AMZN 2022-12-21  0.525736  \n",
       "\n",
       "[6445 rows x 9 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      " 2/80 [..............................] - ETA: 1:14 - loss: 0.0191 - mean_absolute_error: 0.0908WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.2219s vs `on_train_batch_end` time: 1.6948s). Check your callbacks.\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.0022 - mean_absolute_error: 0.0275\n",
      "Epoch 00001: val_loss improved from inf to 0.00035, saving model to results\\2023-01-15_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "80/80 [==============================] - 20s 255ms/step - loss: 0.0022 - mean_absolute_error: 0.0275 - val_loss: 3.4778e-04 - val_mean_absolute_error: 0.0117\n",
      "Epoch 2/5\n",
      "80/80 [==============================] - ETA: 0s - loss: 8.0726e-04 - mean_absolute_error: 0.0195\n",
      "Epoch 00002: val_loss did not improve from 0.00035\n",
      "80/80 [==============================] - 30s 376ms/step - loss: 8.0726e-04 - mean_absolute_error: 0.0195 - val_loss: 3.5993e-04 - val_mean_absolute_error: 0.0120\n",
      "Epoch 3/5\n",
      "80/80 [==============================] - ETA: 0s - loss: 7.0205e-04 - mean_absolute_error: 0.0179\n",
      "Epoch 00003: val_loss improved from 0.00035 to 0.00035, saving model to results\\2023-01-15_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "80/80 [==============================] - 32s 402ms/step - loss: 7.0205e-04 - mean_absolute_error: 0.0179 - val_loss: 3.4729e-04 - val_mean_absolute_error: 0.0126\n",
      "Epoch 4/5\n",
      "80/80 [==============================] - ETA: 0s - loss: 7.4321e-04 - mean_absolute_error: 0.0195\n",
      "Epoch 00004: val_loss did not improve from 0.00035\n",
      "80/80 [==============================] - 31s 386ms/step - loss: 7.4321e-04 - mean_absolute_error: 0.0195 - val_loss: 3.9785e-04 - val_mean_absolute_error: 0.0146\n",
      "Epoch 5/5\n",
      "80/80 [==============================] - ETA: 0s - loss: 7.1624e-04 - mean_absolute_error: 0.0182\n",
      "Epoch 00005: val_loss did not improve from 0.00035\n",
      "80/80 [==============================] - 32s 395ms/step - loss: 7.1624e-04 - mean_absolute_error: 0.0182 - val_loss: 4.4138e-04 - val_mean_absolute_error: 0.0169\n"
     ]
    }
   ],
   "source": [
    "# some tensorflow callbacks\n",
    "checkpointer = ModelCheckpoint(os.path.join(\"results\", model_name + \".h5\"), save_weights_only=True, save_best_only=True, verbose=1)\n",
    "tensorboard = TensorBoard(log_dir=os.path.join(\"logs\", model_name))\n",
    "# train the model and save the weights whenever we see \n",
    "# a new optimal model using ModelCheckpoint\n",
    "history = model.fit(data[\"X_train\"], data[\"y_train\"],\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    epochs=EPOCHS,\n",
    "                    validation_data=(data[\"X_test\"], data[\"y_test\"]),\n",
    "                    callbacks=[checkpointer, tensorboard],\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_graph(test_df):\n",
    "    \"\"\"\n",
    "    This function plots true close price along with predicted close price\n",
    "    with blue and red colors respectively\n",
    "    \"\"\"\n",
    "    plt.plot(test_df[f'true_adjclose_{LOOKUP_STEP}'], c='b')\n",
    "    plt.plot(test_df[f'adjclose_{LOOKUP_STEP}'], c='r')\n",
    "    plt.xlabel(\"Days\")\n",
    "    plt.ylabel(\"Price\")\n",
    "    plt.legend([\"Actual Price\", \"Predicted Price\"])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_df(model, data):\n",
    "    \"\"\"\n",
    "    This function takes the `model` and `data` dict to \n",
    "    construct a final dataframe that includes the features along \n",
    "    with true and predicted prices of the testing dataset\n",
    "    \"\"\"\n",
    "    # if predicted future price is higher than the current, \n",
    "    # then calculate the true future price minus the current price, to get the buy profit\n",
    "    buy_profit  = lambda current, pred_future, true_future: true_future - current if pred_future > current else 0\n",
    "    # if the predicted future price is lower than the current price,\n",
    "    # then subtract the true future price from the current price\n",
    "    sell_profit = lambda current, pred_future, true_future: current - true_future if pred_future < current else 0\n",
    "    X_test = data[\"X_test\"]\n",
    "    y_test = data[\"y_test\"]\n",
    "    # perform prediction and get prices\n",
    "    y_pred = model.predict(X_test)\n",
    "    if SCALE:\n",
    "        y_test = np.squeeze(data[\"column_scaler\"][\"adjclose\"].inverse_transform(np.expand_dims(y_test, axis=0)))\n",
    "        y_pred = np.squeeze(data[\"column_scaler\"][\"adjclose\"].inverse_transform(y_pred))\n",
    "    test_df = data[\"test_df\"]\n",
    "    # add predicted future prices to the dataframe\n",
    "    test_df[f\"adjclose_{LOOKUP_STEP}\"] = y_pred\n",
    "    # add true future prices to the dataframe\n",
    "    test_df[f\"true_adjclose_{LOOKUP_STEP}\"] = y_test\n",
    "    # sort the dataframe by date\n",
    "    test_df.sort_index(inplace=True)\n",
    "    final_df = test_df\n",
    "    # add the buy profit column\n",
    "    final_df[\"buy_profit\"] = list(map(buy_profit, \n",
    "                                    final_df[\"adjclose\"], \n",
    "                                    final_df[f\"adjclose_{LOOKUP_STEP}\"], \n",
    "                                    final_df[f\"true_adjclose_{LOOKUP_STEP}\"])\n",
    "                                    # since we don't have profit for last sequence, add 0's\n",
    "                                    )\n",
    "    # add the sell profit column\n",
    "    final_df[\"sell_profit\"] = list(map(sell_profit, \n",
    "                                    final_df[\"adjclose\"], \n",
    "                                    final_df[f\"adjclose_{LOOKUP_STEP}\"], \n",
    "                                    final_df[f\"true_adjclose_{LOOKUP_STEP}\"])\n",
    "                                    # since we don't have profit for last sequence, add 0's\n",
    "                                    )\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, data):\n",
    "    # retrieve the last sequence from data\n",
    "    last_sequence = data[\"last_sequence\"][-N_STEPS:]\n",
    "    # expand dimension\n",
    "    last_sequence = np.expand_dims(last_sequence, axis=0)\n",
    "    # get the prediction (scaled from 0 to 1)\n",
    "    prediction = model.predict(last_sequence)\n",
    "    # get the price (by inverting the scaling)\n",
    "    if SCALE:\n",
    "        predicted_price = data[\"column_scaler\"][\"adjclose\"].inverse_transform(prediction)[0][0]\n",
    "    else:\n",
    "        predicted_price = prediction[0][0]\n",
    "    return predicted_price,last_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load optimal model weights from results folder\n",
    "model_path = os.path.join(\"results\", model_name) + \".h5\"\n",
    "model.load_weights(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "loss, mae = model.evaluate(data[\"X_test\"], data[\"y_test\"], verbose=0)\n",
    "# calculate the mean absolute error (inverse scaling)\n",
    "if SCALE:\n",
    "    mean_absolute_error = data[\"column_scaler\"][\"adjclose\"].inverse_transform([[mae]])[0][0]\n",
    "else:\n",
    "    mean_absolute_error = mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the final dataframe for the testing set\n",
    "final_df = get_final_df(model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>adjclose</th>\n",
       "      <th>volume</th>\n",
       "      <th>ticker</th>\n",
       "      <th>adjclose_15</th>\n",
       "      <th>true_adjclose_15</th>\n",
       "      <th>buy_profit</th>\n",
       "      <th>sell_profit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1997-08-01</th>\n",
       "      <td>0.117188</td>\n",
       "      <td>0.120833</td>\n",
       "      <td>0.112500</td>\n",
       "      <td>0.120833</td>\n",
       "      <td>0.120833</td>\n",
       "      <td>71760000</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>-0.988445</td>\n",
       "      <td>0.106250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.014583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-08-05</th>\n",
       "      <td>0.116667</td>\n",
       "      <td>0.116667</td>\n",
       "      <td>0.107292</td>\n",
       "      <td>0.110417</td>\n",
       "      <td>0.110417</td>\n",
       "      <td>60648000</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>-1.019741</td>\n",
       "      <td>0.117188</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.006771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-08-08</th>\n",
       "      <td>0.108333</td>\n",
       "      <td>0.115104</td>\n",
       "      <td>0.105208</td>\n",
       "      <td>0.114583</td>\n",
       "      <td>0.114583</td>\n",
       "      <td>44232000</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>-1.058296</td>\n",
       "      <td>0.116927</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.002344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-08-13</th>\n",
       "      <td>0.111458</td>\n",
       "      <td>0.111458</td>\n",
       "      <td>0.108333</td>\n",
       "      <td>0.109896</td>\n",
       "      <td>0.109896</td>\n",
       "      <td>11808000</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>-1.078404</td>\n",
       "      <td>0.127604</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.017708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-08-14</th>\n",
       "      <td>0.108854</td>\n",
       "      <td>0.112500</td>\n",
       "      <td>0.105990</td>\n",
       "      <td>0.107813</td>\n",
       "      <td>0.107813</td>\n",
       "      <td>20832000</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>-1.081232</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.017187</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                open      high       low     close  adjclose    volume ticker  \\\n",
       "1997-08-01  0.117188  0.120833  0.112500  0.120833  0.120833  71760000   AMZN   \n",
       "1997-08-05  0.116667  0.116667  0.107292  0.110417  0.110417  60648000   AMZN   \n",
       "1997-08-08  0.108333  0.115104  0.105208  0.114583  0.114583  44232000   AMZN   \n",
       "1997-08-13  0.111458  0.111458  0.108333  0.109896  0.109896  11808000   AMZN   \n",
       "1997-08-14  0.108854  0.112500  0.105990  0.107813  0.107813  20832000   AMZN   \n",
       "\n",
       "            adjclose_15  true_adjclose_15  buy_profit  sell_profit  \n",
       "1997-08-01    -0.988445          0.106250         0.0     0.014583  \n",
       "1997-08-05    -1.019741          0.117188         0.0    -0.006771  \n",
       "1997-08-08    -1.058296          0.116927         0.0    -0.002344  \n",
       "1997-08-13    -1.078404          0.127604         0.0    -0.017708  \n",
       "1997-08-14    -1.081232          0.125000         0.0    -0.017187  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the future price\n",
    "future_price,last_df = predict(model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90.31858"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "future_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we calculate the accuracy by counting the number of positive profits\n",
    "accuracy_score = (len(final_df[final_df['sell_profit'] > 0]) + len(final_df[final_df['buy_profit'] > 0])) / len(final_df)\n",
    "# calculating total buy & sell profit\n",
    "total_buy_profit  = final_df[\"buy_profit\"].sum()\n",
    "total_sell_profit = final_df[\"sell_profit\"].sum()\n",
    "# total profit by adding sell & buy together\n",
    "total_profit = total_buy_profit + total_sell_profit\n",
    "# dividing total profit by number of testing samples (number of trades)\n",
    "profit_per_trade = total_profit / len(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>adjclose</th>\n",
       "      <th>volume</th>\n",
       "      <th>ticker</th>\n",
       "      <th>date</th>\n",
       "      <th>future</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2022-12-15</th>\n",
       "      <td>0.479986</td>\n",
       "      <td>0.476704</td>\n",
       "      <td>0.473034</td>\n",
       "      <td>88.449997</td>\n",
       "      <td>0.473887</td>\n",
       "      <td>0.036141</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>2022-12-15</td>\n",
       "      <td>0.468042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-16</th>\n",
       "      <td>0.471329</td>\n",
       "      <td>0.473416</td>\n",
       "      <td>0.469029</td>\n",
       "      <td>87.860001</td>\n",
       "      <td>0.470723</td>\n",
       "      <td>0.065677</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>2022-12-16</td>\n",
       "      <td>0.481501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-19</th>\n",
       "      <td>0.467268</td>\n",
       "      <td>0.464296</td>\n",
       "      <td>0.457015</td>\n",
       "      <td>84.919998</td>\n",
       "      <td>0.454959</td>\n",
       "      <td>0.035529</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>2022-12-19</td>\n",
       "      <td>0.509490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-20</th>\n",
       "      <td>0.455618</td>\n",
       "      <td>0.458887</td>\n",
       "      <td>0.456041</td>\n",
       "      <td>85.190002</td>\n",
       "      <td>0.456407</td>\n",
       "      <td>0.031107</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>2022-12-20</td>\n",
       "      <td>0.510455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-21</th>\n",
       "      <td>0.460160</td>\n",
       "      <td>0.462174</td>\n",
       "      <td>0.460803</td>\n",
       "      <td>86.769997</td>\n",
       "      <td>0.464879</td>\n",
       "      <td>0.023845</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>2022-12-21</td>\n",
       "      <td>0.525736</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                open      high       low      close  adjclose    volume  \\\n",
       "2022-12-15  0.479986  0.476704  0.473034  88.449997  0.473887  0.036141   \n",
       "2022-12-16  0.471329  0.473416  0.469029  87.860001  0.470723  0.065677   \n",
       "2022-12-19  0.467268  0.464296  0.457015  84.919998  0.454959  0.035529   \n",
       "2022-12-20  0.455618  0.458887  0.456041  85.190002  0.456407  0.031107   \n",
       "2022-12-21  0.460160  0.462174  0.460803  86.769997  0.464879  0.023845   \n",
       "\n",
       "           ticker       date    future  \n",
       "2022-12-15   AMZN 2022-12-15  0.468042  \n",
       "2022-12-16   AMZN 2022-12-16  0.481501  \n",
       "2022-12-19   AMZN 2022-12-19  0.509490  \n",
       "2022-12-20   AMZN 2022-12-20  0.510455  \n",
       "2022-12-21   AMZN 2022-12-21  0.525736  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stock.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>adjclose</th>\n",
       "      <th>volume</th>\n",
       "      <th>ticker</th>\n",
       "      <th>adjclose_15</th>\n",
       "      <th>true_adjclose_15</th>\n",
       "      <th>buy_profit</th>\n",
       "      <th>sell_profit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2022-12-01</th>\n",
       "      <td>96.989998</td>\n",
       "      <td>97.230003</td>\n",
       "      <td>94.919998</td>\n",
       "      <td>95.500000</td>\n",
       "      <td>95.500000</td>\n",
       "      <td>68488000</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>94.960228</td>\n",
       "      <td>83.790001</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.709999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-05</th>\n",
       "      <td>93.050003</td>\n",
       "      <td>94.059998</td>\n",
       "      <td>90.820000</td>\n",
       "      <td>91.010002</td>\n",
       "      <td>91.010002</td>\n",
       "      <td>71535500</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>94.925110</td>\n",
       "      <td>83.040001</td>\n",
       "      <td>-7.970001</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-08</th>\n",
       "      <td>89.239998</td>\n",
       "      <td>90.860001</td>\n",
       "      <td>87.879997</td>\n",
       "      <td>90.349998</td>\n",
       "      <td>90.349998</td>\n",
       "      <td>73305900</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>92.938148</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>-6.349998</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-12</th>\n",
       "      <td>89.209999</td>\n",
       "      <td>90.580002</td>\n",
       "      <td>87.870003</td>\n",
       "      <td>90.550003</td>\n",
       "      <td>90.550003</td>\n",
       "      <td>61999800</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>91.737053</td>\n",
       "      <td>85.139999</td>\n",
       "      <td>-5.410004</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-16</th>\n",
       "      <td>88.269997</td>\n",
       "      <td>89.349998</td>\n",
       "      <td>86.730003</td>\n",
       "      <td>87.860001</td>\n",
       "      <td>87.860001</td>\n",
       "      <td>146144100</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>91.322525</td>\n",
       "      <td>89.870003</td>\n",
       "      <td>2.010002</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 open       high        low      close   adjclose     volume  \\\n",
       "2022-12-01  96.989998  97.230003  94.919998  95.500000  95.500000   68488000   \n",
       "2022-12-05  93.050003  94.059998  90.820000  91.010002  91.010002   71535500   \n",
       "2022-12-08  89.239998  90.860001  87.879997  90.349998  90.349998   73305900   \n",
       "2022-12-12  89.209999  90.580002  87.870003  90.550003  90.550003   61999800   \n",
       "2022-12-16  88.269997  89.349998  86.730003  87.860001  87.860001  146144100   \n",
       "\n",
       "           ticker  adjclose_15  true_adjclose_15  buy_profit  sell_profit  \n",
       "2022-12-01   AMZN    94.960228         83.790001    0.000000    11.709999  \n",
       "2022-12-05   AMZN    94.925110         83.040001   -7.970001     0.000000  \n",
       "2022-12-08   AMZN    92.938148         84.000000   -6.349998     0.000000  \n",
       "2022-12-12   AMZN    91.737053         85.139999   -5.410004     0.000000  \n",
       "2022-12-16   AMZN    91.322525         89.870003    2.010002     0.000000  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Future price after 15 days is 90.32$\n",
      "huber_loss loss: 0.00034729010076262057\n",
      "Mean Absolute Error: 2.428758751452907\n",
      "Accuracy score: 0.48515625\n",
      "Total buy profit: 320.494463801384\n",
      "Total sell profit: 165.83592261373985\n",
      "Total profit: 486.3303864151238\n",
      "Profit per trade: 0.3799456143868155\n"
     ]
    }
   ],
   "source": [
    "# printing metrics\n",
    "print(f\"Future price after {LOOKUP_STEP} days is {future_price:.2f}$\")\n",
    "print(f\"{LOSS} loss:\", loss)\n",
    "print(\"Mean Absolute Error:\", mean_absolute_error)\n",
    "print(\"Accuracy score:\", accuracy_score)\n",
    "print(\"Total buy profit:\", total_buy_profit)\n",
    "print(\"Total sell profit:\", total_sell_profit)\n",
    "print(\"Total profit:\", total_profit)\n",
    "print(\"Profit per trade:\", profit_per_trade)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot true/pred prices graph\n",
    "plot_graph(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the final dataframe to csv-results folder\n",
    "csv_results_folder = \"csv-results\"\n",
    "if not os.path.isdir(csv_results_folder):\n",
    "    os.mkdir(csv_results_folder)\n",
    "csv_filename = os.path.join(csv_results_folder, model_name + \".csv\")\n",
    "final_df.to_csv(csv_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
